{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RunningModelOnAzureDatabricks.ipynb","provenance":[],"authorship_tag":"ABX9TyOz6TCJtP9v7bAAbUtNmvFz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cVl-z3Oc664P","colab_type":"text"},"source":["First of all, you should have a azure databricks workspace and a cluster to run code.\n","\n","For migrating your local model on azure databricks, first compress your pickle files or model files into .jar files. For this you can also compress file into .zip then  convert it into .jar by writting on comand prompt\n","\n","*mv model.zip model.jar*\n","\n","Now upload this jar into your workspace by clicking import option on your working folder. After this open a notebook and decompress these file into tmp folder of your workspace. "]},{"cell_type":"code","metadata":{"id":"WUr6vVAp63uk","colab_type":"code","colab":{}},"source":["%sh \n","rm -rf /tmp/model\n","mkdir /tmp/model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Nx2D4K38yrI","colab_type":"text"},"source":["Now after creation unzip them in model folder"]},{"cell_type":"code","metadata":{"id":"dC39LbY787UU","colab_type":"code","colab":{}},"source":["import zipfile\n","with zipfile.ZipFile(\"/dbfs/FileStore/jars/idOfYourModelJarFile-forest.jar\",\"r\") as zip_ref:\n","    print(zip_ref)\n","    zip_ref.extractall(\"/tmp/model\")\n","with zipfile.ZipFile(\"/dbfs/FileStore/jars/idOfYourOtherRequiredJar-impColumns.jar\",\"r\") as zip_ref:\n","    print(zip_ref)\n","    zip_ref.extractall(\"/tmp/model\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ibYsavG99HTZ","colab_type":"text"},"source":["load your model in your notebook"]},{"cell_type":"code","metadata":{"id":"j2LdkYcf9DVB","colab_type":"code","colab":{}},"source":["from os import listdir\n","from os.path import isfile, join\n","import pandas as pd\n","from sklearn.externals import joblib\n","\n","#read model from the tmp path\n","model = joblib.load(\"/tmp/model/model.pkl\")\n","impColumns = joblib.load(\"/tmp/model/impColumns.pkl\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHuYqWzd9aRS","colab_type":"text"},"source":["Make prediction on the model"]},{"cell_type":"code","metadata":{"id":"3-_IIJ_i9gqo","colab_type":"code","colab":{}},"source":["import sys\n","from sklearn.externals import joblib\n","import datetime\n","from calendar import monthrange\n","from dateutil.relativedelta import relativedelta\n","import os\n","\n","#Opening and reading file from url or take it from local\n","df_spark = spark.read.format(\"formatofFile\").load(FILEPATH)\n","df = df_spark.toPandas()\n","#df.head()\n","    \n","#making predictions on data    \n","pred = model.predict_proba(df[impColumns]).max(axis=1)\n","pred_class = model.predict(df[impColumns])\n","df_output = pd.DataFrame()\n","df_output['UserId'] = df.UserId\n","df_output = df_output.assign(preb_prob=pred)\n","df_output = df_output.assign(classType=pred_class)\n","print(df_output)\n","\n","\n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MQq8gIxr9_aF","colab_type":"text"},"source":["Uploading outputted file at the location in dbfs"]},{"cell_type":"code","metadata":{"id":"78qBVVNV-CLJ","colab_type":"code","colab":{}},"source":["#Outputting file at required location in dbfs\n","outname = \"test.txt\"\n","outdir = '/dbfs/theLocationWhereYouWantToOutputYourFile/'\n","df_output.to_csv(outdir+outname, index=False, sep='\\t',header=True)\n","print(\"File %s has been upload to\" % outdir)\n","    "],"execution_count":0,"outputs":[]}]}